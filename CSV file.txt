import requests
from bs4 import BeautifulSoup
import csv

def scrape_amazon_products():
    url = "https://www.amazon.com/Best-Sellers-Electronics/zgbs/electronics"

    # Send a GET request to the URL
    response = requests.get(url)

    # Check if the request was successful (status code 200)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all product listings on the page
        product_listings = soup.find_all('div', {'class': 'zg-item-immersion'})

        # Create a CSV file to store the product information
        with open('amazon_products.csv', 'w', newline='', encoding='utf-8') as csv_file:
            fieldnames = ['Name', 'Price', 'Rating']
            writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
            writer.writeheader()

            # Extract and write product information to the CSV file
            for product_listing in product_listings:
                name = product_listing.find('div', {'class': 'p13n-sc-truncate'}).text.strip()
                price = product_listing.find('span', {'class': 'p13n-sc-price'}).text.strip()
                rating = product_listing.find('span', {'class': 'a-icon-alt'}).text.strip()

                # Write the data to the CSV file
                writer.writerow({'Name': name, 'Price': price, 'Rating': rating})

        print("Scraping completed. Product information saved to amazon_products.csv.")
    else:
        print(f"Failed to retrieve the page. Status code: {response.status_code}")

if __name__ == "__main__":
    scrape_amazon_products()
